
"""fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/181eztH3wgcFj9T2VPuZOuITobIZWAt8b
"""


import os
IN_COLAB = "COLAB_" in "".join(os.environ.keys())

if not IN_COLAB:
    !pip install unsloth
    !sudo apt update
    !sudo apt install libcurl4-openssl-dev -y
else:
    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
    !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
    !pip install --no-deps unsloth

from unsloth import FastLanguageModel
import torch
max_seq_length=2048 
load_in_4bit = True  

model,tokenizer=FastLanguageModel.from_pretrained(
    model_name="unsloth/Phi-4",
    max_seq_length=max_seq_length,
    load_in_4bit =load_in_4bit,
)

model=FastLanguageModel.get_peft_model(
    model,
    r=16, 
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
     random_state = 3407,  
    use_rslora = False,  
    loftq_config = None,
)

from datasets import load_dataset


dataset = load_dataset("mrkswe/llmEndpointDatasetConversation_2", split="train")
print(dataset.column_names)  

import json

def parse_conversations(example):
    example['conversations'] = json.loads(example['conversations'])
    return example

dataset = dataset.map(parse_conversations)

from unsloth.chat_templates import standardize_sharegpt

dataset = standardize_sharegpt(dataset)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "phi-4", 
)

def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = [
        tokenizer.apply_chat_template(
            convo, tokenize=False, add_generation_prompt=False
        )
        for convo in convos
    ]
    return { "text": texts }

dataset = dataset.map(formatting_prompts_func, batched=True)

from transformers import DataCollatorForLanguageModeling 
collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)

max_seq_length=2048
def tokenize_function(example):
  return tokenizer(example["text"],truncation=True,padding="max_length",max_length=max_seq_length)

tokenized_dataset=dataset.map(tokenize_function,batched=True)

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = tokenized_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 50,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",  
    ),
)

from unsloth import FastLanguageModel

FastLanguageModel.save_lora(
    model = trainer.model,
    model_name = "helin-lora-phi4"
)

from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

base_model_id = "unsloth/Phi-4"
adapter_id = "helin-lora-phi4"


model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map="auto")
model = PeftModel.from_pretrained(model, adapter_id)

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(tokenizer, chat_template="phi-4")


while True:
    prompt = input("Sen: ")
    if prompt.lower() in ["exit", "quit", "çık"]:
        print("Çıkılıyor...")
        break

    
    chat_prompt = tokenizer.apply_chat_template(
        [{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True
    )

    inputs = tokenizer(chat_prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=200)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("Model:", response)

from huggingface_hub import create_repo, upload_folder

token = "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" 

create_repo("helin-lora-phi4", repo_type="model", private=False, token=token,exist_ok=True)

upload_folder(
    repo_id="Helin02/helin-lora-phi4",      
    folder_path="helin-lora-phi4",          
    repo_type="model",
    commit_message="LoRA fine-tuned Phi-4 model",
    token=token
)
commit_message="LoRA fine-tuned Phi-4 model"

   
    
